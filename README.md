# Policy Q&A Agent (RAG over Internal Policies)

This project implements a lightweight Retrieval-Augmented Generation (RAG) system designed to answer employee questions using a small collection of internal policy documents. The agent retrieves relevant context from local files and generates grounded responses using a local LLM.

## System Capabilities

The system can be run in two ways:
- **Locally** using Python and a virtual environment
- **Containerized** using Docker, connecting to a local LLM served via Ollama

---

## Requirements

### System Requirements
- Windows 10/11 with PowerShell
- Python 3.10+ — https://www.python.org/
- Ollama (local LLM runtime) — https://ollama.com/
- Docker Desktop — https://www.docker.com/products/docker-desktop/ (required only for the containerized version)

### Python Dependencies

All Python dependencies are listed in `requirements.txt` and can be installed using:

```powershell
pip install -r requirements.txt
```

**Key libraries used:**
- **fastapi, uvicorn** — lightweight HTTP API server
- **langchain, langchain_community** — agent and RAG components
- **chromadb** — persistent local vector store
- **sentence-transformers, transformers, torch** — embeddings and model utilities
- **requests** — HTTP calls to the local LLM (Ollama)

---

## Project Structure

```
policy_ai_agent/
├── app.py               # FastAPI application exposing the /ask endpoint
├── agent.py             # Core agent logic: retrieval, prompt construction, LLM call
├── rag.py               # One-time document ingestion + Chroma index creation
├── evaluate.py          # Evaluation script for running test questions
├── test_data.json       # Synthetic test questions + expected answers
├── policies/            # Synthetic internal policy documents (.md)
├── chroma_db/           # Persistent Chroma vector store (generated by rag.py)
├── requirements.txt     # Python dependencies
├── Dockerfile           # Containerization for the FastAPI service
└── README.md            # Project documentation
```

---

## Local Setup and Run Instructions (without Docker)

### Step 1: Create and activate a virtual environment

From the project root:

```powershell
cd policy_ai_agent
python -m venv .venv
```

On Windows PowerShell, activate the environment:

```powershell
.\.venv\Scripts\Activate.ps1
```

You should now see `(.venv)` in front of your prompt.

### Step 2: Install dependencies

```powershell
pip install -r requirements.txt
```

### Step 3: Prepare the vector store (RAG index)

`rag.py` reads all policy files located under `policies/`, splits them into chunks, and creates a persistent Chroma vector store under `chroma_db/`.

Run:

```powershell
python rag.py
```

You should see log messages indicating that documents were loaded, split, embedded, and written to the vector store.

### Step 4: Start the local API

Run the FastAPI service:

```powershell
python app.py
```

This starts Uvicorn on port 8000. Example log output:

```
Uvicorn running on http://0.0.0.0:8000
```

### Step 5: Call the API locally

From a separate PowerShell window (this one does not need the virtual environment):

```powershell
Invoke-WebRequest -Uri "http://127.0.0.1:8000/ask" `
    -Method POST `
    -Body '{"question":"How many days per week can I work remotely?"}' `
    -ContentType "application/json" | Select-Object -ExpandProperty Content
```

**Example response:**

```json
{
  "question": "How many days per week can I work remotely?",
  "answer": "Employees may work remotely up to 3 days per week unless team-specific arrangements differ.",
  "sources": ["remote_work_policy.md"]
}
```

---

## Running with Docker

The Docker image includes:
- A Python 3.10 slim base image
- All dependencies installed from `requirements.txt`
- The application code
- The FastAPI server as the default entrypoint

> **Note:** The container still communicates with a local LLM running on the host (via Ollama). The model itself is not included inside the Docker image.

### Prerequisite: Local LLM via Ollama

Install Ollama and pull a compatible model. Example:

```powershell
ollama pull llama3.2
# or, if you configured a different model in agent.py:
# ollama pull gemma:1b
```

Verify that the model runs:

```powershell
ollama run llama3.2
# type something, then Ctrl+C to exit
```

### Build the Docker image

From the project root:

```powershell
docker build -t policy-agent .
```

### Run the container

The service exposes port 8000. The local LLM endpoint is passed via the `OLLAMA_URL` environment variable.

On Docker Desktop for Windows, the host machine can be reached from inside the container using `host.docker.internal`.

**Example run:**

```powershell
docker run -p 8000:8000 `
    -e OLLAMA_URL="http://host.docker.internal:11434/api/generate" `
    policy-agent
```

**Expected output:**

```
Uvicorn running on http://0.0.0.0:8000
```

### Test the containerized API

From another terminal, using PowerShell:

```powershell
Invoke-WebRequest -Uri "http://127.0.0.1:8000/ask" `
    -Method POST `
    -Body '{"question":"How many days per week can I work remotely?"}' `
    -ContentType "application/json" | Select-Object -ExpandProperty Content
```

Or using curl:

```bash
curl -X POST "http://127.0.0.1:8000/ask" \
     -H "Content-Type: application/json" \
     -d '{"question": "How many days per week can I work remotely?"}'
```

---

## Configuring the Local LLM Endpoint

The LLM call is managed in agent.py. The endpoint URL is read from an environment variable, with a safe default:

import os

OLLAMA_URL = os.getenv(
    "OLLAMA_URL",
    "http://localhost:11434/api/generate",  # default for local (non-Docker) runs
)
MODEL_NAME = "gemma:2b"  # or any model you have pulled in Ollama

5.1. Local Python runs

### Local Python runs

When running `python app.py` directly, the default is used:

```
http://localhost:11434/api/generate
```

This assumes Ollama is running on the same machine with its default port (11434).

### Docker runs

When running inside Docker, `localhost` would refer to the container itself, not the host. To make the container talk to the host's Ollama instance, override `OLLAMA_URL`:

```powershell
docker run -p 8000:8000 `
    -e OLLAMA_URL="http://host.docker.internal:11434/api/generate" `
    policy-agent
```

This pattern also allows switching to a different LLM gateway (e.g., on-prem LLM, Azure-hosted LLM) by simply pointing `OLLAMA_URL` to another HTTP endpoint.

---

## Architecture and Key Design Choices

### High-level Architecture

1. **Client**  
   - Sends a POST request to `/ask` with a question in JSON.

2. **FastAPI layer (`app.py`)**  
   - Exposes a simple HTTP interface.  
   - Delegates the request to `agent.ask()`.

3. **Agent (`agent.py`)**  
   - Calls `load_vector_store()` to access the persistent Chroma index.  
   - Calls `retrieve_context()` to fetch the most relevant chunks for the question.  
   - Builds a prompt that:
     - Includes the retrieved context.  
     - Instructs the LLM to answer only using that context.  
     - Instructs the LLM to return a fixed fallback phrase if no answer exists in the documentation.  
   - Calls the local LLM via HTTP (`requests.post` to `OLLAMA_URL`).  
   - Returns the final answer along with the list of source documents used.

4. **RAG Index (`rag.py` + ChromaDB)**  
   - Loads all `.md` files from `policies/`.  
   - Splits them into overlapping chunks using `RecursiveCharacterTextSplitter`.  
   - Embeds each chunk using `all-MiniLM-L6-v2` from sentence-transformers.  
   - Stores them in a persistent Chroma vector store under `chroma_db/`.

5. **Local LLM (Ollama)**  
   - Receives the prompt and generates an answer.  
   - Completely stateless relative to the rest of the system.

### Key Design Choices

- **RAG instead of pure LLM** — The model never has full "knowledge" of the policies. All answers come only from retrieved context.

- **Short, dense chunks**  
  Chunk size and overlap were tuned to make each chunk self-contained while maintaining semantic continuity.

- **Simple score-based reranking**  
  `similarity_search_with_score` is used, and only the top-N chunks are kept after sorting by distance.  
  This provides lightweight reranking without adding extra models.

- **Explicit “I don’t know” fallback**  
  The agent injects a fixed fallback phrase:  
  > “The information is not provided in the available documentation.”

  The evaluation script checks for this phrase to distinguish correct “I don’t know” responses from hallucinations.

- **Local-only LLM for IP safety**  
  All LLM calls are made to a local Ollama instance.  
  This aligns with on-prem, IP-conscious deployment requirements.

  ## 7. Evaluation (Hallucination Behaviour)

The project includes a small evaluation harness in `evaluate.py` with synthetic test cases in `test_data.json`.

Each test case defines:

- **question**: the user query  
- **expected_keywords**: a list of keywords that should appear in the answer (for questions that should be answerable)  
- **should_answer**: boolean flag  
  - `true` → the agent is expected to provide a concrete answer  
  - `false` → the agent is expected to reply with the fallback “I don’t know” phrase  

### `evaluate.py`:

1. Iterates over all test cases.  
2. Calls `agent.ask(question)` for each.  
3. Checks:  
   - **If `should_answer == true`:**  
     - Answer must contain at least one of the expected keywords  
     - Answer must *not* contain the fallback phrase  
   - **If `should_answer == false`:**  
     - Answer must contain exactly the fallback phrase  
     - Any other answer is counted as a hallucination  
4. Prints a small report:  
   - Number of correct answers  
   - Number of correct “I don’t know” answers  
   - Wrong answers  
   - Hallucinations  

### Run

```bash
python evaluate.py
```


## 8. Known Limitations and Improvement Ideas

### 8.1. Limitations

- **Simple keyword-based evaluation**  
  The evaluation script only checks for keyword presence and the fallback phrase.  
  It cannot measure partial correctness or semantic similarity.

- **No advanced reranking model**  
  Retrieval quality relies purely on Chroma’s distance-based similarity scores,  
  with no cross-encoder or learning-to-rank component.

- **Single local LLM endpoint**  
  All LLM calls go to one Ollama instance with no load balancing,  
  no model switching, and no GPU-aware routing.

- **No authentication or RBAC**  
  The FastAPI service is unsecured (no auth headers or role-based access control).  
  Acceptable for a coding test, but not for production.

### 8.2. Possible Improvements

- **Stronger reranking**  
  Add a second-stage cross-encoder (e.g., MiniLM) on top of the Chroma results  
  to increase retrieval accuracy.

- **Richer evaluation metrics**  
  Expand the test dataset and compute metrics such as precision/recall  
  for answerable vs. unanswerable questions.

- **Safeguarding module**  
  Add a lightweight filter (regex or classifier) to block unethical or unsafe outputs  
  before returning the final answer.

- **Human-in-the-loop feedback**  
  Add a `/feedback` endpoint for thumbs-up/down logging to support  
  future prompt refinement or retrieval improvements.

